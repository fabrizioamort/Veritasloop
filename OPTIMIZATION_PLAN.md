# VeritasLoop Agent Flow Optimization Plan

## Executive Summary

**Current State**: Sequential execution, 40-50 seconds total runtime for 3 debate rounds
**Target State**: Parallel & asynchronous execution, 20-25 seconds total runtime (50% improvement)
**Approach**: Strategic parallelization, lazy research, resource pooling, and asynchronous debate flow

---

## Deep Analysis: Current Bottlenecks

### 1. Sequential Research Phase (9 seconds wasted)

**Current Flow**:
```
extract_claim (2s) â†’ pro_research (4s) â†’ contra_research (5s) â†’ debate...
                     â¬‡ï¸ BLOCKS          â¬‡ï¸ BLOCKS
                     CONTRA waits 4s    Debate waits 9s
```

**Problem**: PRO and CONTRA research are independent but execute sequentially
**Impact**: 4-5 seconds of pure waiting time

### 2. Synchronous Debate Rounds (21-27 seconds wasted)

**Current Flow per Round**:
```
PRO Turn:
  - Search (1s) â†’ LLM (3s) = 4s TOTAL
                              â¬‡ï¸ CONTRA WAITS
CONTRA Turn:
  - Search #1 (1s) â†’ Search #2 (1s) â†’ LLM (3s) = 5s TOTAL
                                                  â¬‡ï¸ PRO WAITS
Repeat 3x = 27 seconds total
```

**Problem**:
- Each agent blocks the other completely
- CONTRA does 2 sequential searches that could be parallel
- No overlap between thinking and searching

**Impact**: 18-21 seconds of waiting across 3 rounds

### 3. Resource Initialization Overhead (3-4 seconds wasted)

**Current Pattern** (found in every node):
```python
# debate.py, lines 33-34, 68-69
llm = get_llm()              # Creates new OpenAI client
tool_manager = ToolManager()  # Creates new cache, API connections
```

**Problem**: 8 node executions Ã— 0.5s initialization = 4 seconds overhead
**Impact**: Connection setup, credential loading, instance creation repeated unnecessarily

### 4. All-or-Nothing Research (Inflexibility)

**Current Pattern**:
```python
# pro_agent.py, line 41
search_results = self.search(claim.core_claim, strategy="institutional")
# Takes top 3 sources immediately (line 83)

# contra_agent.py, lines 70-80
search_results = self.search(search_query, strategy="fact_check_first")
if not is_initial_round:
    more_results = self.search(rebuttal_query, strategy="web_deep_dive")
# Takes top 5 sources immediately (line 102)
```

**Problem**:
- Research happens all at once before any speaking
- Can't adapt research based on opponent's response
- PRO always speaks with full research even when unopposed

**Impact**: Wasted API calls, slower startup, less natural flow

---

## Optimization Strategy: Three-Tier Approach

### TIER 1: Quick Wins (30-40% speedup, Low Risk)
Immediate improvements with minimal architectural changes

### TIER 2: Flow Restructuring (50-60% speedup, Medium Risk)
Fundamental changes to debate orchestration

### TIER 3: Advanced Optimizations (60-70% speedup, High Risk)
Aggressive parallelization and predictive strategies

---

## TIER 1 OPTIMIZATIONS: Quick Wins

### 1.1 Parallel Initial Research Phase

**Change**: Run `pro_research` and `contra_research` in parallel

**Implementation**:
```python
# In graph.py, replace sequential edges:
# OLD:
workflow.add_edge("extract", "pro_research")
workflow.add_edge("pro_research", "contra_research")
workflow.add_edge("contra_research", "pro_node")

# NEW:
workflow.add_edge("extract", "research_parallel")

def research_parallel(state: GraphState) -> dict:
    """Run PRO and CONTRA research concurrently."""
    import asyncio
    from concurrent.futures import ThreadPoolExecutor

    with ThreadPoolExecutor(max_workers=2) as executor:
        pro_future = executor.submit(pro_research_internal, state)
        contra_future = executor.submit(contra_research_internal, state)

        pro_msg = pro_future.result()
        contra_msg = contra_future.result()

    return {
        "messages": [pro_msg, contra_msg]  # operator.add appends both
    }

workflow.add_edge("research_parallel", "pro_node")
```

**Impact**:
- **Before**: 4s (PRO) + 5s (CONTRA) = 9s sequential
- **After**: max(4s, 5s) = 5s parallel
- **Savings**: 4 seconds (10% total runtime reduction)

**Risk**: Low - Research is independent, no data dependencies

**Files to modify**:
- `src/orchestrator/graph.py` (lines 212-217)

---

### 1.2 Parallel CONTRA Search Calls

**Change**: Execute CONTRA's two searches concurrently

**Implementation**:
```python
# In contra_agent.py, think() method
# OLD (lines 70-80):
search_results = self.search(search_query, strategy="fact_check_first")
if not is_initial_round and messages:
    more_results = self.search(rebuttal_query, strategy="web_deep_dive")
    search_results.extend(more_results)

# NEW:
from concurrent.futures import ThreadPoolExecutor

if not is_initial_round and messages:
    # Parallel execution
    with ThreadPoolExecutor(max_workers=2) as executor:
        future1 = executor.submit(self.search, search_query, "fact_check_first", max_searches)
        future2 = executor.submit(self.search, rebuttal_query, "web_deep_dive", max_searches)

        results1 = future1.result()
        results2 = future2.result()
        search_results = results1 + results2
else:
    # Initial round: single search
    search_results = self.search(search_query, "fact_check_first", max_searches)
```

**Impact**:
- **Before**: 1s + 1s = 2s sequential per round
- **After**: max(1s, 1s) = 1s parallel per round
- **Savings**: 1s Ã— 3 rounds = 3 seconds (7% total runtime reduction)

**Risk**: Low - Searches are independent queries

**Files to modify**:
- `src/agents/contra_agent.py` (lines 66-80)

---

### 1.3 Resource Pooling (Singleton Pattern)

**Change**: Share LLM and ToolManager instances across all nodes

**Implementation**:
```python
# Create new file: src/utils/resource_pool.py
from functools import lru_cache
from src.utils.tool_manager import ToolManager
from src.utils.claim_extractor import get_llm

@lru_cache(maxsize=1)
def get_shared_tool_manager():
    """Singleton ToolManager instance."""
    return ToolManager()

@lru_cache(maxsize=1)
def get_shared_llm():
    """Singleton LLM instance."""
    return get_llm()

# In graph.py, replace lines 132-133:
# OLD:
tool_manager = ToolManager()
llm = get_llm()

# NEW:
from src.utils.resource_pool import get_shared_tool_manager, get_shared_llm
tool_manager = get_shared_tool_manager()
llm = get_shared_llm()

# In debate.py, replace lines 33-34, 68-69:
# OLD:
llm = get_llm()
tool_manager = ToolManager()

# NEW:
from src.utils.resource_pool import get_shared_tool_manager, get_shared_llm
llm = get_shared_llm()
tool_manager = get_shared_tool_manager()
```

**Impact**:
- **Before**: 8 initializations Ã— 0.5s = 4s overhead
- **After**: 1 initialization Ã— 0.5s = 0.5s overhead
- **Savings**: 3.5 seconds (8% total runtime reduction)

**Risk**: Low - Stateless resources, thread-safe by default

**Files to modify**:
- Create `src/utils/resource_pool.py`
- `src/orchestrator/graph.py` (lines 132-133)
- `src/orchestrator/debate.py` (lines 33-34, 68-69)

---

### TIER 1 TOTAL IMPACT

| Optimization | Time Saved | Implementation Effort |
|--------------|------------|----------------------|
| Parallel Research | 4s | 2 hours |
| Parallel CONTRA Searches | 3s | 1 hour |
| Resource Pooling | 3.5s | 1.5 hours |
| **TIER 1 TOTAL** | **10.5s (26% reduction)** | **4.5 hours** |

**New Runtime**: 40-50s â†’ 30-40s

---

## TIER 2 OPTIMIZATIONS: Flow Restructuring

### 2.1 Lazy Research Pattern (PRO Starts Without Research)

**Concept**: PRO makes opening statement based only on claim text, no search

**Rationale**:
- Real debates start with opening statements, not evidence dumps
- Allows immediate engagement (feels faster to user)
- PRO can do "just in time" research when challenged
- More natural conversational flow

**Implementation**:
```python
# In graph.py, modify workflow:
# OLD:
workflow.add_edge("extract", "research_parallel")
workflow.add_edge("research_parallel", "pro_node")

# NEW:
workflow.add_edge("extract", "pro_opening")  # No research
workflow.add_edge("pro_opening", "contra_research")
workflow.add_edge("contra_research", "debate_loop")

def pro_opening(state: GraphState) -> dict:
    """PRO's initial statement WITHOUT research."""
    pro_personality = state.get('pro_personality', 'ASSERTIVE')
    pro_agent = ProAgent(llm, tool_manager, personality=pro_personality)

    # Skip search, generate opening based on claim alone
    message = pro_agent.opening_statement(state)  # New method
    return {"messages": [message], "round_count": 1}

# In pro_agent.py, add new method:
def opening_statement(self, state: GraphState) -> DebateMessage:
    """Generate opening statement without research."""
    claim = state['claim']
    language = state.get('language', 'Italian')

    prompt = f"""
    Claim: {claim.core_claim}
    Category: {claim.category}

    You are opening a debate in support of this claim.
    Make a compelling opening statement based on the claim itself and common knowledge.
    Do NOT cite specific sources yet - this is your opening position.

    IMPORTANT: Your output must be in {language}.
    """

    response = self.llm.invoke([
        SystemMessage(content=self.system_prompt),
        HumanMessage(content=prompt)
    ])

    return DebateMessage(
        round=0,
        agent=AgentType.PRO,
        message_type=MessageType.ARGUMENT,
        content=response.content,
        sources=[],  # No sources yet
        confidence=60.0  # Lower confidence without research
    )
```

**Modified Flow**:
```
OLD:
extract (2s) â†’ pro_research (4s) â†’ contra_research (5s) â†’ debate...
                                                          â¬‡ï¸ 11s to first message

NEW:
extract (2s) â†’ pro_opening (3s) â†’ contra_research (5s) â†’ debate...
                                                         â¬‡ï¸ 7s to first message
```

**Impact**:
- **Time to First Message**: 11s â†’ 5s (6 second improvement in perceived speed)
- **Total Runtime**: Similar, but feels much faster
- **User Experience**: Immediate engagement, more natural

**Trade-off**: PRO's first statement has no sources (acceptable for opening)

**Files to modify**:
- `src/orchestrator/graph.py` (workflow edges, add pro_opening node)
- `src/agents/pro_agent.py` (add opening_statement method)

---

### 2.2 Incremental Research (On-Demand Source Gathering)

**Concept**: Start with 1-2 sources, fetch more only when needed

**Rationale**:
- Most debates don't need 3-5 sources per message
- First round often establishes positions, later rounds need evidence
- Adaptive: deep research only when opponent challenges

**Implementation**:
```python
# Add to GraphState in schemas.py:
class GraphState(TypedDict):
    # ... existing fields ...
    research_depth: int  # 0=no research, 1=shallow, 2=deep

# Modify agent search to respect depth:
def think(self, state: GraphState) -> DebateMessage:
    depth = state.get('research_depth', 1)

    if depth == 0:
        # No research, claim-based only
        search_results = []
        max_sources = 0
    elif depth == 1:
        # Shallow: 1-2 sources
        search_results = self.search(claim.core_claim, strategy="institutional")[:2]
        max_sources = 2
    else:  # depth == 2
        # Deep: 3-5 sources
        search_results = self.search(claim.core_claim, strategy="institutional")[:5]
        max_sources = 5

# Add adaptive depth logic in graph.py:
def adaptive_research_depth(state: GraphState) -> dict:
    """Increase research depth if confidence drops."""
    messages = state['messages']

    if not messages:
        return {"research_depth": 0}  # First message: no research

    last_msg = messages[-1]
    if last_msg.confidence < 50:
        return {"research_depth": 2}  # Low confidence: deep research
    else:
        return {"research_depth": 1}  # Normal: shallow research
```

**Impact**:
- **API Calls**: Reduced by ~40% (fewer searches when not needed)
- **Runtime**: 2-3s saved on rounds that don't need deep research
- **Quality**: Better when needed, lighter when not

**Files to modify**:
- `src/models/schemas.py` (add research_depth to GraphState)
- `src/agents/pro_agent.py` (modify think() to respect depth)
- `src/agents/contra_agent.py` (modify think() to respect depth)
- `src/orchestrator/graph.py` (add adaptive logic)

---

### 2.3 Asynchronous Agent Pipeline

**Concept**: While one agent speaks (LLM running), the other can search/prepare

**Rationale**:
- LLM inference (3s) and API search (1s) can overlap
- PRO's LLM call can run while CONTRA searches
- CONTRA's LLM call can run while PRO searches for next round

**Implementation**:
```python
# Redesign debate rounds with pipelining:

async def pipelined_debate_round(state: GraphState) -> dict:
    """Execute PRO and CONTRA with overlapping operations."""
    import asyncio

    # Phase 1: PRO speaks, CONTRA searches (parallel)
    async def pro_speak():
        return await run_in_executor(pro_agent.think, state)

    async def contra_prepare():
        # Start CONTRA's search while PRO is thinking
        await asyncio.sleep(1.5)  # Wait for PRO to incorporate previous CONTRA msg
        return await run_in_executor(contra_agent.pre_search, state)

    pro_msg, contra_prep = await asyncio.gather(pro_speak(), contra_prepare())

    # Update state with PRO's message
    state['messages'].append(pro_msg)

    # Phase 2: CONTRA speaks (using pre-fetched results)
    contra_msg = contra_agent.think_with_prep(state, contra_prep)

    return {
        "messages": [pro_msg, contra_msg],
        "round_count": state['round_count'] + 1
    }
```

**Timeline Visualization**:
```
OLD (Sequential):
PRO:    |--search(1s)--|--LLM(3s)--|                    = 4s
CONTRA:                              |--search(1s)--|--LLM(3s)--| = 5s
TOTAL: 9s per round

NEW (Pipelined):
PRO:    |--search(1s)--|--LLM(3s)--|
CONTRA:         |--search(1s)--|--LLM(3s)--|
                â¬†ï¸ Starts while PRO is in LLM
TOTAL: 5s per round (4s saved per round)
```

**Impact**:
- **Per Round**: 9s â†’ 5s (4 seconds saved)
- **3 Rounds**: 27s â†’ 15s (12 seconds saved)
- **Total Runtime**: 40-50s â†’ 28-38s

**Complexity**: High - requires async/await, careful state management

**Files to modify**:
- `src/orchestrator/debate.py` (complete rewrite for async)
- `src/orchestrator/graph.py` (support async nodes)
- Requires LangGraph async support (may need upgrade)

---

### TIER 2 TOTAL IMPACT

| Optimization | Time Saved | User Experience | Effort |
|--------------|------------|-----------------|--------|
| Lazy Research | 4s + faster perceived start | â­â­â­â­â­ | 3 hours |
| Incremental Research | 2-3s | â­â­â­ | 4 hours |
| Async Pipeline | 12s | â­â­â­â­ | 8 hours |
| **TIER 2 TOTAL** | **18-19s (45% reduction)** | | **15 hours** |

**New Runtime**: 30-40s â†’ 15-25s (combining with Tier 1)

---

## TIER 3 OPTIMIZATIONS: Advanced Strategies

### 3.1 Predictive Pre-Caching

**Concept**: Pre-fetch likely searches based on claim category

**Implementation**:
```python
def extract_claim(state: GraphState) -> dict:
    extracted = extract_from_text(current_claim.raw_input)

    # Trigger background pre-caching
    asyncio.create_task(pre_cache_likely_searches(extracted))

    return {"claim": extracted}

async def pre_cache_likely_searches(claim: Claim):
    """Background task to warm cache."""
    likely_queries = [
        f"official statistics {claim.core_claim}",
        f"fact check {claim.core_claim}",
        f"{claim.category} verification"
    ]

    # Fire and forget
    for query in likely_queries:
        await search_async(query)
```

**Impact**: 1-2s saved on cache hits (30-50% hit rate expected)

---

### 3.2 Speculative Execution

**Concept**: Start CONTRA's round before PRO finishes (rollback if needed)

**Risk**: High - May waste resources if PRO's message changes CONTRA's approach

**Impact**: 2-3s saved per round (but wasteful on rollbacks)

---

### 3.3 Batch LLM Calls

**Concept**: Send PRO and CONTRA prompts to LLM simultaneously

**Risk**: High - LLM may not maintain distinct personas

**Impact**: 2-3s saved per round

---

### TIER 3 TOTAL IMPACT

| Optimization | Time Saved | Risk Level | Effort |
|--------------|------------|------------|--------|
| Predictive Caching | 1-2s | Medium | 4 hours |
| Speculative Execution | 6-9s | High | 8 hours |
| Batch LLM | 6-9s | High | 6 hours |
| **TIER 3 TOTAL** | **13-20s (25-40% reduction)** | | **18 hours** |

**New Runtime**: 15-25s â†’ 10-15s (combining all tiers)

---

## Recommended Implementation Path

### Phase 1: Quick Wins (Week 1)
**Goal**: 30-40% speedup with low risk

1. âœ… Parallel Initial Research (Day 1-2)
2. âœ… Parallel CONTRA Searches (Day 2-3)
3. âœ… Resource Pooling (Day 3-4)
4. ğŸ§ª Testing & Validation (Day 5-7)

**Deliverables**:
- 10.5s faster runtime
- No functionality changes
- Backwards compatible

---

### Phase 2: Flow Improvements (Week 2-3)
**Goal**: 50%+ speedup with moderate risk

1. âœ… Lazy Research (Day 8-10)
2. âœ… Incremental Research (Day 11-14)
3. ğŸ§ª A/B Testing (Day 15-17)
4. ğŸ“Š User Feedback (Day 18-21)

**Deliverables**:
- More natural debate flow
- Faster perceived performance
- Adaptive research depth

---

### Phase 3: Advanced Optimizations (Week 4+)
**Goal**: 60-70% speedup (optional)

1. ğŸ”¬ Async Pipeline (Week 4-5)
2. ğŸ”¬ Predictive Caching (Week 6)
3. ğŸ“Š Performance Benchmarking (Week 7)

**Deliverables**:
- Maximum performance
- Complex async architecture
- Requires thorough testing

---

## Implementation Details by File

### Files Requiring Modification

| File | Tier 1 | Tier 2 | Tier 3 | Complexity |
|------|--------|--------|--------|------------|
| `src/orchestrator/graph.py` | âœ… Major | âœ… Major | âœ… Major | â­â­â­â­â­ |
| `src/orchestrator/debate.py` | âœ… Minor | âœ… Major | âœ… Rewrite | â­â­â­â­ |
| `src/agents/pro_agent.py` | - | âœ… Moderate | âœ… Minor | â­â­â­ |
| `src/agents/contra_agent.py` | âœ… Minor | âœ… Moderate | âœ… Minor | â­â­â­ |
| `src/models/schemas.py` | - | âœ… Minor | âœ… Minor | â­â­ |
| `src/utils/resource_pool.py` | âœ… New File | - | - | â­ |
| `src/utils/tool_manager.py` | - | - | âœ… Moderate | â­â­â­ |

---

## Testing Strategy

### Performance Testing
```python
# tests/test_performance.py
import time
import pytest

def test_research_parallelization():
    """Verify parallel research is faster than sequential."""
    # Sequential baseline
    start = time.time()
    run_sequential_research(state)
    sequential_time = time.time() - start

    # Parallel implementation
    start = time.time()
    run_parallel_research(state)
    parallel_time = time.time() - start

    assert parallel_time < sequential_time * 0.7  # At least 30% faster

def test_lazy_research_speed():
    """Ensure lazy research reduces time to first message."""
    start = time.time()
    result = run_debate_with_lazy_research(claim)
    first_message_time = time.time() - start

    assert first_message_time < 7.0  # Target: under 7s
```

### Quality Testing
```python
def test_quality_maintained():
    """Ensure optimizations don't degrade verdict quality."""
    test_claims = load_test_claims()

    for claim in test_claims:
        baseline_verdict = run_baseline_pipeline(claim)
        optimized_verdict = run_optimized_pipeline(claim)

        # Verdicts should match
        assert baseline_verdict.verdict == optimized_verdict.verdict

        # Confidence should be within 10 points
        assert abs(baseline_verdict.confidence - optimized_verdict.confidence) < 10
```

---

## Risk Assessment & Mitigation

### Low Risk Changes (Tier 1)
- âœ… Parallel research: Independent operations
- âœ… Resource pooling: Stateless singletons
- âš ï¸ **Mitigation**: Feature flag to disable parallelization if issues

### Medium Risk Changes (Tier 2)
- âš ï¸ Lazy research: May reduce initial argument quality
- âš ï¸ Incremental research: Adaptive logic complexity
- âš ï¸ **Mitigation**: A/B testing, quality metrics, rollback plan

### High Risk Changes (Tier 3)
- ğŸš¨ Async pipeline: Race conditions, state management
- ğŸš¨ Speculative execution: Wasted resources
- ğŸš¨ **Mitigation**: Extensive testing, gradual rollout, monitoring

---

## Performance Metrics to Track

### Speed Metrics
- Total execution time (extract â†’ verdict)
- Time to first message
- Time per debate round
- API call latency

### Quality Metrics
- Verdict accuracy (against baseline)
- Confidence score consistency
- Source quality (reliability distribution)
- User satisfaction (frontend feedback)

### Resource Metrics
- API calls per verification
- Cache hit rate
- Memory usage (resource pooling)
- Concurrent operation success rate

---

## Alternative Approaches Considered

### âŒ Agent Pre-Warming
**Idea**: Keep agents running in background
**Rejected**: Memory overhead, doesn't solve sequential flow

### âŒ Multiple Judge Evaluations
**Idea**: Judge evaluates after each round
**Rejected**: Doesn't speed up debate, adds overhead

### âœ… WebSocket Progress Updates (Complementary)
**Idea**: Stream agent status to frontend
**Accepted**: Improves perceived performance without code changes

### âŒ GPU Acceleration for LLM
**Idea**: Local LLM inference on GPU
**Rejected**: Out of scope, depends on API provider

---

## Success Criteria

### Must Have (Tier 1)
- âœ… 25-30% runtime reduction
- âœ… No degradation in verdict quality
- âœ… Backwards compatible API
- âœ… All existing tests pass

### Should Have (Tier 2)
- âœ… 50% runtime reduction
- âœ… Improved user experience (lazy loading)
- âœ… Adaptive research depth
- âœ… Maintains debate quality

### Nice to Have (Tier 3)
- âœ… 60%+ runtime reduction
- âœ… Predictive caching
- âœ… Fully async pipeline
- âœ… Real-time streaming updates

---

## Conclusion & Recommendations

### Immediate Next Steps (Recommended)
1. **Implement Tier 1 (Week 1)**: Low risk, high impact quick wins
2. **User Testing**: Validate that speed improvements don't hurt quality
3. **Decide on Tier 2**: Based on user feedback and business priorities

### Long-Term Vision
The combination of Tier 1 + Tier 2 optimizations provides the best balance:
- **50% faster** (40-50s â†’ 20-25s)
- **Better UX** (immediate engagement, natural flow)
- **Reasonable complexity** (4-6 weeks implementation)

### Not Recommended (Yet)
- **Tier 3 Async Pipeline**: Wait for LangGraph native async support
- **Speculative Execution**: Risk/reward ratio too high

---

## Appendix: Detailed Flow Diagrams

### Current Flow (Sequential)
```
Timeline (seconds): 0    5    10   15   20   25   30   35   40   45   50
                    |----|----|----|----|----|----|----|----|----|----|
Extract:            [==]
PRO Research:           [====]
CONTRA Research:             [=====]
PRO Round 1:                      [====]
CONTRA Round 1:                        [=====]
PRO Round 2:                                 [====]
CONTRA Round 2:                                   [=====]
PRO Round 3:                                            [====]
CONTRA Round 3:                                              [=====]
Judge:                                                            [===]

TOTAL: 47 seconds (all sequential)
```

### Optimized Flow (Tier 1 + Tier 2)
```
Timeline (seconds): 0    5    10   15   20   25   30
                    |----|----|----|----|----|----|
Extract:            [==]
PRO Opening:            [===] (no research)
CONTRA Research:            [=====]
Round 1 (pipelined):         [=====]
Round 2 (pipelined):              [=====]
Round 3 (pipelined):                   [=====]
Judge:                                      [===]

TOTAL: 25 seconds (47% reduction)

Key Improvements:
- PRO/CONTRA research parallel: saved 4s
- PRO opening without research: saved 4s (perceived)
- Pipelined rounds: saved 12s
- Resource pooling: saved 3.5s
```

---

## Questions for Stakeholders

Before implementation, clarify:

1. **Quality vs Speed Trade-off**: Is 50% faster worth potentially 5-10% lower confidence scores on initial statements?

2. **User Experience Priority**: What matters more - total runtime or time to first message?

3. **Resource Budget**: Are we optimizing for API call cost or wall-clock time?

4. **Backwards Compatibility**: Can we change the GraphState schema (research_depth field)?

5. **Testing Coverage**: What's the minimum test coverage required before production deploy?

---

**Document Version**: 1.0
**Date**: 2025-12-28
**Author**: Claude (Analysis Agent)
**Status**: Ready for Review
**Next Action**: Stakeholder review and Tier 1 implementation approval
